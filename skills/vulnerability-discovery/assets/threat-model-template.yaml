# LLM Threat Model Template
# Systematic attack surface analysis

system_info:
  name: "[System Name]"
  type: "[API/Chatbot/Agent/Plugin]"
  model: "[GPT-4/Claude/etc]"
  deployment: "[Cloud/On-prem/Hybrid]"

attack_surface:
  input_vectors:
    - name: User Prompts
      risk: high
      controls: []
      tests:
        - prompt_injection
        - jailbreaking
        - encoding_evasion

    - name: System Prompts
      risk: critical
      controls: []
      tests:
        - prompt_extraction
        - instruction_override

    - name: External Data
      risk: high
      controls: []
      tests:
        - indirect_injection
        - data_poisoning

    - name: API Parameters
      risk: medium
      controls: []
      tests:
        - parameter_tampering
        - rate_limit_bypass

  output_vectors:
    - name: Generated Content
      risk: high
      concerns:
        - harmful_content
        - pii_leakage
        - hallucinations

    - name: Function Calls
      risk: critical
      concerns:
        - unauthorized_actions
        - data_exfiltration

threat_actors:
  - name: Malicious Users
    motivation: Bypass safety
    capability: medium
    targets: [safety_mechanisms, content_filters]

  - name: Competitors
    motivation: Extract IP
    capability: high
    targets: [system_prompts, training_data]

  - name: Researchers
    motivation: Find vulnerabilities
    capability: high
    targets: [all_attack_surfaces]

risk_matrix:
  likelihood_scale: [rare, unlikely, possible, likely, certain]
  impact_scale: [negligible, minor, moderate, major, severe]

vulnerability_categories:
  - LLM01: Prompt Injection
  - LLM02: Insecure Output Handling
  - LLM03: Training Data Poisoning
  - LLM04: Model Denial of Service
  - LLM05: Supply Chain Vulnerabilities
  - LLM06: Sensitive Information Disclosure
  - LLM07: Insecure Plugin Design
  - LLM08: Excessive Agency
  - LLM09: Overreliance
  - LLM10: Model Theft
